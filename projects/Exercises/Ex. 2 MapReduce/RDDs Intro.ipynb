{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b58d82-1bb7-497a-a19d-8bae4545ec94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e7ab62-b7ad-49e5-893a-a1fd041a6ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/07 13:21:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"mapReduce\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdaf9e1-bc70-40ed-b6e0-4da19503876c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5,6,7,8,9,0]).map(lambda x: x **2).sum() # Here We are creating an RDD and we're performing a calculation on its elements (Square) and getting the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6d3ed3-b33a-4231-91f7-4230542202ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cat', 'rat', 'dog', 'bird', 'lizard', 'chicken', 'cat', 'rat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = [\"cat\",\"rat\",\"dog\",\"bird\",\"lizard\",\"chicken\",\"cat\",\"rat\"]\n",
    "wordsRDD = sc.parallelize(word_list,3)\n",
    "print(type(wordsRDD))\n",
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24743b4f-e531-4154-a2f7-9c2822106942",
   "metadata": {},
   "source": [
    "#### RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8774a7-65f5-4ec6-a373-d604b6a82324",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "cats\n",
      "['cats', 'rats']\n",
      "['cats', 'rats', 'dogs', 'birds', 'lizards', 'chickens', 'cats', 'rats']\n"
     ]
    }
   ],
   "source": [
    "def make_plural(w):    # Create a function that adds an s to the element that it is passed\n",
    "    return w + \"s\"\n",
    "\n",
    "print(make_plural(\"cat\"))   # Use of the function\n",
    "# Here we're gonna TRANSFORM our wordsRDD into a new RDD\n",
    "plural_wordRDD = wordsRDD.map(make_plural)\n",
    "print(plural_wordRDD.first()) # Here the first is anaction because it returns a value \n",
    "print(plural_wordRDD.take(2))\n",
    "print(plural_wordRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443aaba-4072-4b90-ae05-d46aead8b2c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccab812c-dbc0-4233-a90c-7df9d74c94fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('rat', 1), ('dog', 1), ('bird', 1), ('lizard', 1), ('chicken', 1), ('cat', 1), ('rat', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda d:(d,1))\n",
    "print(wordPairs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad617ef7-50ff-403b-a4a3-4a7d57afb575",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d66a289-c774-4d7f-9879-03b9135faf0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('horse', 1), ('rat', 2), ('dog', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"cat\",\"rat\",\"dog\",\"horse\",\"cat\",\"rat\"]\n",
    "wordsRDD = sc.parallelize(word_list,3)\n",
    "wordsCountCollected = (wordsRDD.map(lambda x :(x,1)).reduceByKey(lambda a,b : a+b).collect() )\n",
    "print(wordsCountCollected)\n",
    "\n",
    "# .map() = Creating a (key,value) pair where the keys are the words and the values are 1 \n",
    "# .reduceByKey() = Apply the redudeByKey func where all values with the same key are grouped and a reduction operation is performed\n",
    "# .collect() = The output value is retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ef095-1cea-47b8-b6f5-3de147922c26",
   "metadata": {},
   "source": [
    "# Explanation on the reduceByKey\n",
    "\n",
    "This function operates on a key-value pair RDD (Resilient Distributed Dataset), where each element in the RDD is a pair (key, value). The main purpose of reduceByKey() is to perform a reduction operation on elements with the same key. It combines values for each key using a specified function and returns an RDD of key-value pairs where each key is unique.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. Grouping by Key: First, it groups together all the values that have the same key. This step is done in parallel across the Spark cluster.\n",
    "\n",
    "2. Applying the Reduction Operation: Then, for each group of values with the same key, it applies a specified function (usually a commutative and associative function) to reduce those values to a single value.\n",
    "\n",
    "3. Output: Finally, it returns an RDD where each unique key is associated with a single value, which is the result of the reduction operation applied to the values with that key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb43119-c3f6-4015-8499-b750df65263d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('color', ['orange', 'yellow', 'black', 'red', 'green', 'blue']), ('fruit', ['pineapple', 'tangerine', 'apple', 'bananna', 'orange', 'mango'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# In this example We-re gonna Concatenate on a list\n",
    "\n",
    "data  = [\n",
    "    (\"fruit\",[\"apple\",\"bananna\",\"orange\"]),\n",
    "    (\"color\",[\"red\",\"green\",\"blue\"]),\n",
    "    (\"fruit\",[\"pineapple\",\"tangerine\"]),\n",
    "    (\"color\",[\"black\"]),\n",
    "    (\"color\",[\"orange\",\"yellow\"]),\n",
    "    (\"fruit\",[\"mango\"])\n",
    "]\n",
    "rdd_test = sc.parallelize(data,4)\n",
    "concatenated_rdd = rdd_test.reduceByKey(lambda x,y : x+y).collect()\n",
    "print(concatenated_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0032b8a-f13a-4739-a725-ab7756d11f8e",
   "metadata": {},
   "source": [
    "# Using cache\n",
    "\n",
    "As we know spark is lazy which means that operations are recorder but only executed when actions are performed such as collect() and count() and so on. it's important to remember that operations return an atual item to the program being a number .count() or a list with .collect().\n",
    "\n",
    "Becase the fact that operations run from the start, they may be inefficient and in many cases we might need to cache the result the first time an operation is run on and RDD so it can be referenced by other operations without having to execute the whole of the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86ec06a7-dec8-4dc7-9c42-b824a548ba3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[43] at readRDDFromFile at PythonRDD.scala:289\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Create an RDD\n",
    "random_words = [\"letter\",\"box\",\"magician\",\"fan\",\"desktop\"]\n",
    "random_wordsRDD = sc.parallelize(random_words)\n",
    "print(random_wordsRDD)\n",
    "print(random_wordsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a8761bf-b226-4c53-813f-f56a8360eb43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When I execute this code, it will be ran from the start\n",
    "random_wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99a14bb9-535d-4f22-b382-59d75790b0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's cache the \n",
    "random_wordsRDD.cache() \n",
    "# We need to rerun again from the start but know that we have give the instruction to cache. it will store the result\n",
    "random_wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f08f092-0bde-420e-bb59-043bc8d783c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When running this again : random_wordsRDD.count() , the \"sc.parallelize(random_words)\" expression won't be run again\n",
    "random_wordsRDD.count()\n",
    "#  Where is this useful: it is when you have branching parts or loops, so that you dont do things again and again. \n",
    "#  Spark, being \"lazy\" will rerun the chain again. So cache or persist serves as a checkpoint, breaking the RDD chain or the lineage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b84d8-1492-4d4d-a824-7f6dfa808052",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60131ec8-a3bf-45ed-90be-74f83ce05f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 'mammal', 'rat': 'mammal', 'dog': 'mammal', 'horse': 'mammal', 'heron': 'bird', 'owl': 'bird', 'eagle': 'bird'}\n",
      "6 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Using SparkRDD operations and python in tandem\n",
    "\n",
    "#Python \n",
    "birds_list = [\"heron\",\"owl\",\"eagle\"]\n",
    "animals_list = birds_list + word_list\n",
    "\n",
    "animals_dict = {}\n",
    "for i in word_list:\n",
    "    animals_dict[i] = \"mammal\"\n",
    "for i in birds_list:\n",
    "    animals_dict[i] = \"bird\"\n",
    "print(animals_dict)\n",
    "\n",
    "#SparkRDD Operations\n",
    "animsRDD = sc.parallelize(animals_list,4)\n",
    "animsRDD.cache()\n",
    "mammals_count = animsRDD.filter(lambda x : animals_dict[x] == \"mammal\").count()\n",
    "birds_count = animsRDD.filter(lambda y: animals_dict[y] ==\"bird\").count()\n",
    "print(mammals_count,birds_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7faaf3cf-4894-42ad-87ab-eab40900e9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
