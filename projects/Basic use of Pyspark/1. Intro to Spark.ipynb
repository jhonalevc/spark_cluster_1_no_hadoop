{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd3b7ea-1682-4f76-bf30-a98a89fff6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\",\"Intro\")\n",
    "# Spark Context allows you to work with the low level features of Spark and the core functions ( RDD Operations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8e12b1-464a-4567-b9dc-2f8e3a0c208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkIntro\").getOrCreate()\n",
    "# Spark Session is for high level opertions. it allows you to work with structured data in the form of dataframes or Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb1b2c-4eeb-4824-90e0-fbbcf0f7a314",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1968b63f-c0d1-473f-8143-b53dec3cfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,0]\n",
    "rdd1 = sc.parallelize(data)\n",
    "# Convert a python listo into an RDD in SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a391f413-2723-4502-b8ca-5281e76d2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rdd transformations  - Keep in mind that they are considered lazy\n",
    "# All RDD transformations create a new RDD (They are inmutable Objects ) \n",
    "squared_rdd = rdd1.map(lambda x : x ** 2 )\n",
    "even_rdd = rdd1.filter(lambda x : x%2 ==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5abcea-fcff-448a-92be-9ef94c200cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rdd Actions - Collect, count, reduce ... The actions return a result to the prgram or write data to external source\n",
    "# RDD Actions return results to the program\n",
    "collected_data = squared_rdd.collect()\n",
    "num_elements = even_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e176d31-e5ec-4430-9857-76f32a92f4cf",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85d0b187-1663-4927-bca3-1f75f739d4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------+\n",
      "|     Name|Age|  Salary|\n",
      "+---------+---+--------+\n",
      "|Alejandro| 28|13500000|\n",
      "|     Sara| 30| 4500000|\n",
      "|    Mauro| 35|13500000|\n",
      "+---------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a Dataframe \n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"Name\",StringType(),True),\n",
    "        StructField(\"Age\",IntegerType(),True),\n",
    "        StructField(\"Salary\",IntegerType(),True)\n",
    "    ]\n",
    ")\n",
    "data = [(\"Alejandro\",28,13500000),(\"Sara\",30,4500000),(\"Mauro\",35,13500000 )]\n",
    "df1 = spark.createDataFrame(data,schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815151c7-b25d-4c3c-9328-b8c45db6fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Car_Name: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Selling_Price: double (nullable = true)\n",
      " |-- Present_Price: double (nullable = true)\n",
      " |-- Kms_Driven: integer (nullable = true)\n",
      " |-- Fuel_Type: string (nullable = true)\n",
      " |-- Seller_Type: string (nullable = true)\n",
      " |-- Transmission: string (nullable = true)\n",
      " |-- Owner: integer (nullable = true)\n",
      "\n",
      "+--------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "|Car_Name|Year|Selling_Price|Present_Price|Kms_Driven|Fuel_Type|Seller_Type|Transmission|Owner|\n",
      "+--------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "|    ritz|2014|         3.35|         5.59|     27000|   Petrol|     Dealer|      Manual|    0|\n",
      "|     sx4|2013|         4.75|         9.54|     43000|   Diesel|     Dealer|      Manual|    0|\n",
      "|    ciaz|2017|         7.25|         9.85|      6900|   Petrol|     Dealer|      Manual|    0|\n",
      "| wagon r|2011|         2.85|         4.15|      5200|   Petrol|     Dealer|      Manual|    0|\n",
      "|   swift|2014|          4.6|         6.87|     42450|   Diesel|     Dealer|      Manual|    0|\n",
      "+--------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading Csv Files\n",
    "data_file = \"/home/jovyan/work/Jupyter_Jobs/data_training/car data.csv\"\n",
    "df2 = spark.read.csv(data_file,header=True,inferSchema =True)\n",
    "df2.printSchema()\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39cb76f9-2616-4041-9d6c-8410f328ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|_corrupt_record|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|              [|       NULL|      NULL|       NULL|      NULL|   NULL|\n",
      "|           NULL|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|           NULL|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading_json_file\n",
    "path_json_file = \"/home/jovyan/work/Jupyter_Jobs/data/iris.json\"\n",
    "df_json = spark.read.json(path_json_file)\n",
    "df_json.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3597be46-97ac-4163-a39c-316478656080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|Name| Age|Status|\n",
      "+----+----+------+\n",
      "|   A|NULL| 10000|\n",
      "|   C|  12|  NULL|\n",
      "|   B|   3| 14500|\n",
      "|   A|   3|  9400|\n",
      "+----+----+------+\n",
      "\n",
      "+----+---+------+\n",
      "|Name|Age|Status|\n",
      "+----+---+------+\n",
      "|   A|  6| 10000|\n",
      "|   C| 12|  NULL|\n",
      "|   B|  3| 14500|\n",
      "|   A|  3|  9400|\n",
      "+----+---+------+\n",
      "\n",
      "+----+---+------+\n",
      "|Name|Age|Status|\n",
      "+----+---+------+\n",
      "|   A|  6| 10000|\n",
      "|   C| 12| 14500|\n",
      "|   B|  3| 14500|\n",
      "|   A|  3|  9400|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cleaning data with Missing Values\n",
    "data_with_missing = [(\"A\",None,10000), (\"C\",12,None),(\"B\",3,14500),(\"A\",3,9400)]\n",
    "df_missing = spark.createDataFrame(data_with_missing, [\"Name\",\"Age\",\"Status\"])\n",
    "df_missing.show()\n",
    "#We're gonna fill the age column with the mean\n",
    "mean_age = df_missing.select(\"Age\").agg({\"Age\":\"avg\"}).collect()[0][0]\n",
    "df_clean = df_missing.na.fill(mean_age,subset =[\"Age\"])\n",
    "df_clean.show()\n",
    "#let's fill the other column with the max value\n",
    "df_clean = df_clean.na.fill(\n",
    "    df_clean.select(\"Status\").agg({\"Status\":\"max\"}).collect()[0][0],\n",
    "    subset = [\"Status\"]\n",
    ")\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f042c20-9b51-4e10-bcce-d0f258d11c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+--------------+\n",
      "|Name|Age|Status|      Features|\n",
      "+----+---+------+--------------+\n",
      "|   A|  6| 10000| [6.0,10000.0]|\n",
      "|   C| 12| 14500|[12.0,14500.0]|\n",
      "|   B|  3| 14500| [3.0,14500.0]|\n",
      "|   A|  3|  9400|  [3.0,9400.0]|\n",
      "+----+---+------+--------------+\n",
      "\n",
      "+----+---+------+--------------+--------------------+\n",
      "|Name|Age|Status|      Features|     Scaled_Features|\n",
      "+----+---+------+--------------+--------------------+\n",
      "|   A|  6| 10000| [6.0,10000.0]|[0.33333333333333...|\n",
      "|   C| 12| 14500|[12.0,14500.0]|           [1.0,1.0]|\n",
      "|   B|  3| 14500| [3.0,14500.0]|           [0.0,1.0]|\n",
      "|   A|  3|  9400|  [3.0,9400.0]|           (2,[],[])|\n",
      "+----+---+------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feature Scaling\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler, VectorAssembler\n",
    "\n",
    "# First We will create a new column called features using VectorAssembler with the values From Age and Status\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\"Age\",\"Status\"],\n",
    "    outputCol=\"Features\"\n",
    ")\n",
    "data_for_scaling = assembler.transform(df_clean)\n",
    "data_for_scaling.show()\n",
    "# Create a Min Max Scaler object\n",
    "scaler_min_max = MinMaxScaler(inputCol = \"Features\",outputCol = \"Scaled_Features\")\n",
    "# Apply the min max scaler\n",
    "scaled_min_max = scaler_min_max.fit(data_for_scaling).transform(data_for_scaling)\n",
    "# Show the result\n",
    "scaled_min_max.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc9fad42-fc2d-4b84-99ee-3cb095071347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Scaled_Features=DenseVector([0.0, -0.7548]))\n",
      "+----+---+------+--------------+--------------------+\n",
      "|Name|Age|Status|      Features|     Scaled_Features|\n",
      "+----+---+------+--------------+--------------------+\n",
      "|   A|  6| 10000| [6.0,10000.0]|[0.0,-0.754829412...|\n",
      "|   C| 12| 14500|[12.0,14500.0]|[1.41421356237309...|\n",
      "|   B|  3| 14500| [3.0,14500.0]|[-0.7071067811865...|\n",
      "|   A|  3|  9400|  [3.0,9400.0]|[-0.7071067811865...|\n",
      "+----+---+------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standard_scaler = StandardScaler(\n",
    "    inputCol=\"Features\",\n",
    "    outputCol=\"Scaled_Features\",\n",
    "    withStd =True,\n",
    "    withMean =True)\n",
    "scaled_standard = standard_scaler.fit(data_for_scaling).transform(data_for_scaling)\n",
    "print(scaled_standard.select(\"Scaled_Features\").collect()[0])\n",
    "scaled_standard.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
